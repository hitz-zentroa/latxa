{
  "pipe_parallel_size": 0,
  "model_parallel_size": 2,

  "optimizer":
    {
      "type": "Adam",
      "params": { "lr": 0.0001, "betas": [0.9, 0.95], "eps": 1.0e-8 },
    },

  "train_micro_batch_size_per_gpu": 8,
  "gradient_accumulation_steps": 8,

  "gradient_clipping": 1.0,
  "weight_decay": 0.1,
  "hidden_dropout": 0,
  "attention_dropout": 0,

  "train_iters": 1000,
  "lr_decay_iters": 1000,
  "lr_decay_style": "cosine",
  "decay_lr_to": 0.033,
  "warmup_iters": 50,

  "wandb_run_name": "13B_LLaMA-Eus",
}
